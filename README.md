# IPMI - LLM specilized on social security legislation



---

## **ğŸ“˜ Model Architecture Overview (Simple Explanation)**

This project fine-tunes a **Qwen 2.5â€“1.5B** language model and turns it into a *careful, critical, technical assistant*.
The goal is to make the model better at reasoning about:

* **Python code**
* **Linux commands**
* **Chat interactions**
* **Excel formulas**
* **Crypto/market data** (BTCâ€“USDT)
* **General critical thinking** (avoid hallucinations)

Although several training scripts exist, they all follow the **same overall structure**, described below.

---

## **1. Base Model (â€œThe Brainâ€)**

We start with:

**Qwen2.5-1.5B-Instruct**
â€” a small, fast, instruction-tuned language model.

It is loaded in **4-bit mode** so it fits on normal GPUs and trains efficiently.

Sometimes we add **LoRA adapters**, which lets us train only small parts of the model instead of the whole thing.

---

## **2. Training Data (â€œWhat the Model Learns Fromâ€)**

The model learns from **examples** taken from:

* Python tasks
* Linux command tasks
* Chat datasets (GPT-5 style)
* Excel error-correction datasets
* Crypto datasets
* Critical thinking datasets

Some scripts also use **distillation**, where a larger â€œteacher modelâ€ generates synthetic answers.
The smaller Qwen model then learns by imitating those answers.

---

## **3. Unified Format (â€œHow All Examples Look the Sameâ€)**

Even though the data comes from different sources, every example is rewritten into a **simple text format** that includes:

* A short **system instruction** (ex: â€œYou are a critical Python assistantâ€¦â€)
* A **domain tag** (ex: `[DOMÃNIO: PYTHON]`)
* A **user question or command**
* The **correct answer**, explanation, or reference solution

This makes all training data look similar, which helps the model learn consistent behavior.

Example:

```
VocÃª Ã© uma assistente crÃ­tica de Python.
[DOMÃNIO: PYTHON]
Problema: <text>
SoluÃ§Ã£o de referÃªncia: <code>
```

---

## **4. Tokenization & Dataset Creation**

Each formatted example is converted into numbers the model can understand (tokens).

We create a dataset where:

* `input_ids` = what the model reads
* `labels` = what the model should learn to predict
* Padding is added so they fit nicely into batches

This is standard **causal language modeling**.

---

## **5. The Training Loop (HuggingFace Trainer)**

All training scripts use the same routine:

1. Load the Qwen model in 4-bit
2. (Optional) Add LoRA adapters
3. Load the dataset
4. Train using HuggingFaceâ€™s `Trainer` with:

   * Small batch sizes
   * Gradient accumulation
   * 8-bit optimizer
   * fp16/bf16 support
5. Save the fine-tuned model into `output_dir/`

No matter which script you choose, the training process follows this pattern.

---

## **6. Variants of the Training Pipeline**

Your project includes several training scripts:

### **A. Distillation**

Uses a big teacher model to generate synthetic answers, then trains Qwen on them.

### **B. Multi-Domain (full)**

Trains on Python + Linux + Chat + Crypto (and sometimes Excel).

Good for building a **general critical technical assistant**.

### **C. Multi-Domain Light**

Only Python + Linux.
Faster, simpler, smaller dataset.

Ideal for small GPUs and quick testing.

---

## **7. What You Get at the End**

The output is a **fine-tuned LLM** that:

* Reads code and points out mistakes
* Analyzes Linux commands with security awareness
* Answers like a careful assistant, not a hallucinating one
* Handles technical tasks in multiple domains
* Avoids blindly agreeing with the user
* Follows your custom templates and tone of voice

This gives you a **compact, specialized, safety-aware technical model** based on Qwen.

---

## **Want a â€œHow to Trainâ€ section or example commands?**

I can add:

* Installation instructions
* How to run each training script
* Recommended hardware
* Example training commands
* Inference examples
* A â€œContributingâ€ section for collaborators

Just tell me.


---
# Technical Description 

## **ğŸ“Œ Training Pipeline Overview**

This repository implements a complete **LLM fine-tuning pipeline** for Qwen-2.5 models using multiple strategies:

* **Knowledge distillation** (teacher â†’ student)
* **Multi-domain supervised fine-tuning (SFT)**
* **Lightweight domain-focused training (Python + Linux)**
* **Optional LoRA adapters** for low-VRAM setups
* **4-bit quantized model loading** for consumer GPUs

The goal is to produce models that are **more critical, more precise, safer, and more technically reliable** across several domains (Python, Linux, Chat, Excel, Crypto, etc.).

---

## **ğŸ§± 1. Data Layer**

The pipeline supports two types of data:

### **A. HuggingFace Datasets**

Depending on the training script, the following datasets are used:

* **openai/openai_humaneval** â€“ Python reasoning and canonical solutions
* **shikhardadhich/linux_commands** â€“ Linux terminal instructions
* **ytz20/LMSYS-Chat-GPT-5-Chat-Response** â€“ Real GPT-5 chat responses
* **WinkingFace/CryptoLM-Bitcoin-BTC-USDT** â€“ Crypto market data
* **Excel formula datasets** â€“ Error correction and formula reasoning
* **NoHallucinations / ArgumentMining** â€“ Critical thinking datasets

These datasets provide **diverse real-world tasks** for the model to learn.

### **B. Synthetic Distillation Data**

In the distillation script, a **larger teacher model** generates high-quality answers for curated prompts.
These responses are saved as `.txt` and later used to train a smaller student model.

---

## **ğŸ§¹ 2. Normalization & Prompt Templates**

All datasets are converted into a **unified one-text-per-line** format containing:

* Domain identification
* User question
* Reference or generated answer
* A system prompt enforcing **critical reasoning**, **error detection**, and **safe behavior**

Example format:

```
[DOMÃNIO: PYTHON] Problema: <problem> SoluÃ§Ã£o de referÃªncia: <solution>
```

This step ensures consistency across heterogeneous datasets and reinforces the desired model personality.

---

## **ğŸ“¦ 3. Dataset Construction**

After normalization:

1. All texts are merged into a single list
2. A **CausalTextDataset** converts each sample into:

   * `input_ids`
   * `attention_mask`
   * `labels` (same as `input_ids` for causal LM training)
3. A **custom collator** pads inputs correctly to build training batches

Different versions use either:

* `tokenizer.pad`
* or manual padding with `pad_sequence` (more stable across HF versions)

---

## **ğŸ§  4. Model Loading**

All training scripts load **Qwen2.5-1.5B-Instruct** in **4-bit quantization**, enabling fine-tuning on affordable GPUs.

LoRA is supported via:

```python
from peft import LoraConfig, get_peft_model
```

This enables:

* Low VRAM usage
* Fast fine-tuning
* Small, mergeable adapter weights

---

## **âš™ï¸ 5. Training Loop (HF Trainer)**

All models are trained using **HuggingFace Trainer**, with settings such as:

* `optim="paged_adamw_8bit"` for memory efficiency
* Evaluation per epoch
* Optional fp16 / bf16
* Gradient accumulation for larger effective batch sizes
* Saving best model checkpoints

Typical configuration:

```python
TrainingArguments(
    num_train_epochs=1â€“3,
    per_device_train_batch_size=1â€“2,
    gradient_accumulation_steps=4â€“8,
    learning_rate=1e-4 to 2e-4,
    save_strategy="epoch",
)
```

---

## **ğŸ¯ 6. Output Models**

Depending on the script used, the pipeline yields:

### **A. Distilled Model**

Student model trained on teacher-generated synthetic data.

### **B. Multi-Domain Critical Model**

Trained on Python + Linux + Chat + Crypto (and optionally Excel + critical-thinking datasets).

### **C. Lightweight Model**

Trained only on Python + Linux for fast iterations and limited VRAM.

All models are saved to `output_dir/` along with the tokenizer.

---

## **ğŸ” Summary Diagram**

```
flowchart TD
    A[Data Sources<br>Python Â· Linux Â· Chat Â· Crypto Â· Excel] --> B[Normalization & Templates]

    B --> C1[Distillation Pipeline]
    B --> C2[Multi-Domain SFT]
    B --> C3[Light Python+Linux]

    C1 --> D1[Synthetic TXT]
    D1 --> E1[Qwen 4-bit + LoRA]
    E1 --> F1[HF Trainer]
    F1 --> G1[Distilled Model]

    C2 --> E2[Qwen 4-bit + LoRA]
    E2 --> F2[HF Trainer]
    F2 --> G2[Critical Multi-Domain Model]

    C3 --> E3[Qwen 4-bit + LoRA]
    E3 --> F3[HF Trainer]
    F3 --> G3[Light Model]
```

---

## **ğŸ“˜ What This Pipeline Enables**

* Training **full LLMs on consumer GPUs**
* Producing models with **strong critical reasoning**
* Domain-specialized models (Python, Linux, Crypto)
* Experiments with distillation and SFT
* Extremely modular: each domain can be added or removed easily
* Clean dataset architecture suitable for scaling

---




| Script                                 | Synthetic Generation?   | Domains                        | Purpose                      |
| -------------------------------------- | ----------------------- | ------------------------------ | ---------------------------- |
| **train_distilled_llm_qwen_falcon.py** | Yes (teacher â†’ student) | Excel, Python, Arch            | Distillation                 |
| **optimized.py**                       | No                      | Excel, Python, Linux, Critical | Multi-domain SFT             |
| **optimized2.py**                      | No                      | Python, Linux, Chat, Crypto    | Larger multi-domain          |
| **optimized3.py**                      | No                      | Same as v2 (safer JSON)        | Stable version               |
| **critical_multi_domain.py**           | No                      | Python, Linux, Chat, Crypto    | Critical reasoning           |
| **multi_light.py**                     | No                      | Python, Linux                  | Lightweight trainer          |
| **multi_light2.py**                    | No                      | Python, Linux                  | Lightweight + manual padding |
| **python_linux.py**                    | No                      | Python, Linux                  | Minimalist trainer           |
| **python_linux2.py**                   | No                      | Python, Linux                  | Minimalist + manual padding  |





                                      ALL TRAINERS
                                            â”‚
                                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚              DATA SOURCES (HF)                   â”‚
                   â”‚  Humaneval | Linux Commands | Excel | Chat | Crypto  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                     DATA CLEANING + TEXT UNIFICATION LAYER
                        (normalize_one_line, domain prompts)
                                            â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                 â”‚                 â”‚
                          â–¼                 â–¼                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  DISTILLATION FAMILY   â”‚ â”‚ MULTI-DOMAIN FULL      â”‚ â”‚ MULTI-DOMAIN LIGHT      â”‚
        â”‚                        â”‚ â”‚ (Python/Linux/Chat/    â”‚ â”‚ (Python + Linux only)   â”‚
        â”‚ train_distilled_llm_   â”‚ â”‚  Crypto)               â”‚ â”‚                         â”‚
        â”‚ qwen_falcon.py         â”‚ â”‚                        â”‚ â”‚ train_qwen_multi_light.pyâ”‚
        â”‚                        â”‚ â”‚ train_distilled_llm_   â”‚ â”‚ train_qwen_multi_light2.pyâ”‚
        â”‚ (Teacher Model â†’       â”‚ â”‚ qwen_optimized.py       â”‚ â”‚ train_qwen_python_linux.pyâ”‚
        â”‚  Synthetic Data â†’      â”‚ â”‚ train_distilled_llm_    â”‚ â”‚ train_qwen_python_linux2.pyâ”‚
        â”‚  Student Fine-Tune)    â”‚ â”‚ qwen_optimized2.py      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ train_distilled_llm_    â”‚
                 â”‚                 â”‚ qwen_optimized3.py      â”‚
                 â”‚                 â”‚ train_qwen_critical_     â”‚
                 â”‚                 â”‚ multi_domain.py          â”‚
                 â–¼                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        SYNTHETIC DATA (TXT)
                 â”‚
                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ SyntheticTextDataset      â”‚
   â”‚  + Collator               â”‚  (shared structure across all families)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
       TOKENIZER + QWEN MODEL LOAD
       (4-bit quantization, optional LoRA)
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ HF Trainer (train/eval)  â”‚
         â”‚ eval per epoch or steps  â”‚
         â”‚ bf16/fp16, 8-bit optim   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         SAVED MODEL OUTPUT DIR

